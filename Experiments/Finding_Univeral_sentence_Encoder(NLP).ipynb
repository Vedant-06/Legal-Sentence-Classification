{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finding Univeral sentence Encoder(NLP).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "4U5OxI2KEfNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdB8HYZ8ElXM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd /content/drive/My Drive/PROJECT - NLP/semantic-segmentation/data/text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UydA3WtAEmzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install \"tensorflow_hub>=0.6.0\"\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot8utSOxFA-q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import seaborn as sns\n",
        "import io"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtVrgQWCIXA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "module_url = \"https://tfhub.dev/google/universal-sentence-encoder/2\" \n",
        "embed = hub.Module(module_url)\n",
        "Files = os.listdir()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AooL9yG2K9_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = '/content/drive/My Drive/PROJECT - NLP/semantic-segmentation/data/text'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQ_DKFYTJtOh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for filename in Files:\n",
        "  with io.open(os.path.join(path,filename),'r') as f:\n",
        "    file = f.read()\n",
        "  s = []\n",
        "  for i in range(len(file.split('\\n'))):\n",
        "    if len(file.split('\\n')[i].split('\\t')) == 2:\n",
        "      s.append(file.split('\\n')[i].split('\\t')[0])\n",
        "  with tf.Session() as session:\n",
        "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
        "    message_embeddings = session.run(embed(s))\n",
        "  print('Saving to '+filename)\n",
        "  fi = open(os.path.join(path+'/pretrained-embeddings/',filename), \"a\")\n",
        "  np.savetxt(os.path.join(path,'pretrained-embeddings/',filename),message_embeddings,delimiter=',',newline='\\n')\n",
        "  #fi.write(s[1]+'\\n')\n",
        "  fi.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CDJn4E0TF2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}